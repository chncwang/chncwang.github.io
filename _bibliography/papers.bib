---
---

@article{wang2020cue,
  abbr={arXiv},
  title={Cue-word Driven Neural Response Generation with a Shrinking Vocabulary},
  author={Wang, Qiansheng and Liu, Yuxin and Lv, Chengguo and Wang, Zhen and Fu, Guohong},
  journal={arXiv preprint arXiv:2010.04927},
  abstract={Open-domain response generation is the task of generating sensible and informative responses to the source sentence. However, neural models tend to generate safe and meaningless responses.  While cue-word introducing approaches encourage responses with concrete semantics and have shown tremendous potential, they still fail to explore diverse responses during decoding. In this paper, we propose a novel but natural approach that can produce multiple cue-words during decoding, and then uses the produced cue-words to drive decoding and shrinks the decoding vocabulary. Thus the neural generation model can explore the full space of responses and discover informative ones with efficiency. Experimental results show that our approach significantly outperforms several strong baseline models with much lower decoding complexity. Especially, our approach can converge to concrete semantics more efficiently during decoding.},
  year={2020},
  html={https://arxiv.org/abs/2010.04927},
  pdf={https://arxiv.org/pdf/2010.04927.pdf},
  selected={true}
}

@inproceedings{wang2020unseen,
  abbr={IJCNN},
  title={Unseen Target Stance Detection with Adversarial Domain Generalization},
  author={Wang, Zhen and Wang, Qiansheng and Lv, Chengguo and Cao, Xue and Fu, Guohong},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2020},
  organization={IEEE},
  abstract={Although stance detection has made great progress in the past few years, it is still facing the problem of unseen targets. In this study, we investigate the domain difference between targets and thus incorporate attention-based conditional encoding with adversarial domain generalization to perform unseen target stance detection. Experimental results show that our approach achieves new state-of-the-art performance on the SemEval-2016 dataset, demonstrating the importance of domain difference between targets in unseen target stance detection.},
  html={https://ieeexplore.ieee.org/abstract/document/9206635},
  pdf={https://arxiv.org/pdf/2010.05471.pdf},
  selected={true}
}

@article{wang2019n3ldg,
  abbr={NLPCC},
  title={N3LDG: A Lightweight Neural Network Library for Natural Language Processing},
  author={Wang, Qiansheng and Yu, Nan and Zhang, Meishan and Han, Zijia and Fu, Guohong},
  journal={Acta Scientiarum Naturalium Universitatis Pekinenis},
  volume={55},
  number={1},
  pages={113--119},
  year={2019},
  publisher={Acta Scientiarum Naturalium Universitatis Pekinenis},
  abstract={The authors propose a neural network library N3LDG for natural language processing. N3LDG constructs computation graphs dynamically, and then executes the computation graphs using dynamic batching. Experiments show that N3LDG can efficiently construct and execute computation graphs when training various neural networks (i.e., CNN, Bi-LSTM, and Tree-LSTM). Moreover, N3LDG achieves better training speed than PyTorch on both the CPU and GPU.},
  html={https://kns.cnki.net/kcms/detail/11.2442.N.20180822.2013.015.html},
  pdf={http://tcci.ccf.org.cn/conference/2018/papers/CN143.pdf},
  selected={false}
}

@article{zhang2019end,
  title={End-to-end neural opinion extraction with a transition-based model},
  author={Zhang, Meishan and Wang, Qiansheng and Fu, Guohong},
  journal={Information Systems},
  volume={80},
  pages={56--63},
  year={2019},
  publisher={Elsevier},
  abstract={Fine-grained opinion extraction has received increasing interests in the natural language processing community. It usually involves several subtasks. Recently, joint methods and neural models have been investigated by several studies, achieving promising performance by using graph-based models such as conditional random field. In this work, we propose a novel end-to-end neural model alternatively for joint opinion extraction, by using a transition-based framework. First, we exploit multi-layer bi-directional long short term memory (LSTM) networks to encode the input sentences, and then decode incrementally based on partial output results dominated by a transition system. We use global normalization and beam search for training and decoding. Experiments on a standard benchmark show that the proposed end-to-end model can achieve competitive results compared with the state-of-the-art neural models of opinion extraction.},
  html={https://www.sciencedirect.com/science/article/abs/pii/S0306437918301182},
  selected={false}
}

@inproceedings{han2019chinese,
  abbr={IALP},
  title={Chinese Spelling Check based on Sequence Labeling},
  author={Han, Zijia and Lv, Chengguo and Wang, Qiansheng and Fu, Guohong},
  booktitle={2019 International Conference on Asian Language Processing (IALP)},
  pages={373--378},
  year={2019},
  organization={IEEE}
}
